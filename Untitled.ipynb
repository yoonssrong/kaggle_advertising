{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9b5e38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72c7912c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape: (800, 12)\n",
      "Test dataset shape: (200, 12)\n"
     ]
    }
   ],
   "source": [
    "CSV_HEADER = [\n",
    "    \"Daily Time Spent on Site\",\n",
    "    \"Age\",\n",
    "    \"Area Income\",\n",
    "    \"Daily Internet Usage\",\n",
    "    \"Male\",\n",
    "    \"Country\",\n",
    "    \"region\",\n",
    "    \"region_incomeLevel\",\n",
    "    \"month\",\n",
    "    \"day\",\n",
    "    \"hour\",\n",
    "    \"Clicked on Ad\"\n",
    "]\n",
    "\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "print(f\"Train dataset shape: {train_data.shape}\")\n",
    "print(f\"Test dataset shape: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9de4a614",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_file = \"train.csv\"\n",
    "test_data_file = \"test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fba68bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Daily Time Spent on Site</th>\n",
       "      <th>Age</th>\n",
       "      <th>Area Income</th>\n",
       "      <th>Daily Internet Usage</th>\n",
       "      <th>Male</th>\n",
       "      <th>Country</th>\n",
       "      <th>region</th>\n",
       "      <th>region_incomeLevel</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>Clicked on Ad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>76.90</td>\n",
       "      <td>28</td>\n",
       "      <td>66107.84</td>\n",
       "      <td>212.67</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76.99</td>\n",
       "      <td>31</td>\n",
       "      <td>56729.78</td>\n",
       "      <td>244.34</td>\n",
       "      <td>1</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57.64</td>\n",
       "      <td>57</td>\n",
       "      <td>45580.92</td>\n",
       "      <td>133.81</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48.86</td>\n",
       "      <td>35</td>\n",
       "      <td>62463.70</td>\n",
       "      <td>128.37</td>\n",
       "      <td>1</td>\n",
       "      <td>180</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38.91</td>\n",
       "      <td>33</td>\n",
       "      <td>56369.74</td>\n",
       "      <td>150.80</td>\n",
       "      <td>1</td>\n",
       "      <td>166</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>74.15</td>\n",
       "      <td>29</td>\n",
       "      <td>54806.18</td>\n",
       "      <td>245.89</td>\n",
       "      <td>1</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>76.28</td>\n",
       "      <td>33</td>\n",
       "      <td>52686.47</td>\n",
       "      <td>254.34</td>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>55.46</td>\n",
       "      <td>37</td>\n",
       "      <td>42078.89</td>\n",
       "      <td>108.10</td>\n",
       "      <td>0</td>\n",
       "      <td>206</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>66.49</td>\n",
       "      <td>20</td>\n",
       "      <td>56884.74</td>\n",
       "      <td>202.16</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>81.22</td>\n",
       "      <td>53</td>\n",
       "      <td>34309.24</td>\n",
       "      <td>223.09</td>\n",
       "      <td>1</td>\n",
       "      <td>184</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Daily Time Spent on Site  Age  Area Income  Daily Internet Usage  Male  \\\n",
       "0                       76.90   28     66107.84                212.67     0   \n",
       "1                       76.99   31     56729.78                244.34     1   \n",
       "2                       57.64   57     45580.92                133.81     1   \n",
       "3                       48.86   35     62463.70                128.37     1   \n",
       "4                       38.91   33     56369.74                150.80     1   \n",
       "..                        ...  ...          ...                   ...   ...   \n",
       "795                     74.15   29     54806.18                245.89     1   \n",
       "796                     76.28   33     52686.47                254.34     0   \n",
       "797                     55.46   37     42078.89                108.10     0   \n",
       "798                     66.49   20     56884.74                202.16     0   \n",
       "799                     81.22   53     34309.24                223.09     1   \n",
       "\n",
       "     Country  region  region_incomeLevel  month  day  hour  Clicked on Ad  \n",
       "0         51       2                   1      4   22     8              0  \n",
       "1        106       0                   0      6   22     7              0  \n",
       "2         57       3                   5      3   15     3              1  \n",
       "3        180       3                   1      1    4     0              1  \n",
       "4        166       1                   3      7   13     7              1  \n",
       "..       ...     ...                 ...    ...  ...   ...            ...  \n",
       "795      103       2                   1      1   10     2              0  \n",
       "796       83       0                   0      2   15     7              0  \n",
       "797      206       1                   1      2   21    23              1  \n",
       "798       86       0                   0      6   10    11              0  \n",
       "799      184       2                   1      1    7    13              0  \n",
       "\n",
       "[800 rows x 12 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e47b219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Daily Time Spent on Site</th>\n",
       "      <th>Age</th>\n",
       "      <th>Area Income</th>\n",
       "      <th>Daily Internet Usage</th>\n",
       "      <th>Male</th>\n",
       "      <th>Country</th>\n",
       "      <th>region</th>\n",
       "      <th>region_incomeLevel</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>Clicked on Ad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68.95</td>\n",
       "      <td>35</td>\n",
       "      <td>61833.90</td>\n",
       "      <td>256.09</td>\n",
       "      <td>0</td>\n",
       "      <td>215</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69.47</td>\n",
       "      <td>26</td>\n",
       "      <td>59785.94</td>\n",
       "      <td>236.50</td>\n",
       "      <td>0</td>\n",
       "      <td>184</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88.91</td>\n",
       "      <td>33</td>\n",
       "      <td>53852.85</td>\n",
       "      <td>208.36</td>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66.00</td>\n",
       "      <td>48</td>\n",
       "      <td>24593.33</td>\n",
       "      <td>131.76</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>74.53</td>\n",
       "      <td>30</td>\n",
       "      <td>68862.00</td>\n",
       "      <td>221.51</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>40.18</td>\n",
       "      <td>29</td>\n",
       "      <td>50760.23</td>\n",
       "      <td>151.96</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>45.17</td>\n",
       "      <td>48</td>\n",
       "      <td>34418.09</td>\n",
       "      <td>132.07</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>39.87</td>\n",
       "      <td>48</td>\n",
       "      <td>47929.83</td>\n",
       "      <td>139.34</td>\n",
       "      <td>1</td>\n",
       "      <td>136</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>70.29</td>\n",
       "      <td>31</td>\n",
       "      <td>56974.51</td>\n",
       "      <td>254.65</td>\n",
       "      <td>1</td>\n",
       "      <td>156</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>76.58</td>\n",
       "      <td>46</td>\n",
       "      <td>41884.64</td>\n",
       "      <td>258.26</td>\n",
       "      <td>0</td>\n",
       "      <td>228</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Daily Time Spent on Site  Age  Area Income  Daily Internet Usage  Male  \\\n",
       "0                       68.95   35     61833.90                256.09     0   \n",
       "1                       69.47   26     59785.94                236.50     0   \n",
       "2                       88.91   33     53852.85                208.36     0   \n",
       "3                       66.00   48     24593.33                131.76     1   \n",
       "4                       74.53   30     68862.00                221.51     1   \n",
       "..                        ...  ...          ...                   ...   ...   \n",
       "195                     40.18   29     50760.23                151.96     0   \n",
       "196                     45.17   48     34418.09                132.07     1   \n",
       "197                     39.87   48     47929.83                139.34     1   \n",
       "198                     70.29   31     56974.51                254.65     1   \n",
       "199                     76.58   46     41884.64                258.26     0   \n",
       "\n",
       "     Country  region  region_incomeLevel  month  day  hour  Clicked on Ad  \n",
       "0        215       4                   3      3   27     0              0  \n",
       "1        184       2                   1      3   13    20              0  \n",
       "2        145       1                   3      1   28    20              0  \n",
       "3         12       1                   1      3    7     1              1  \n",
       "4         82       3                   5      4   18     9              0  \n",
       "..       ...     ...                 ...    ...  ...   ...            ...  \n",
       "195       94       1                   1      6   25     4              1  \n",
       "196      160       1                   1      1   27    14              1  \n",
       "197      136       3                   5      6   13     6              1  \n",
       "198      156       0                   0      3   30    14              0  \n",
       "199      228       2                   3      1    3    16              0  \n",
       "\n",
       "[200 rows x 12 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d83b8adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Male'] = list(map(str, train_data['Male']))\n",
    "train_data['month'] = list(map(str, train_data['month']))\n",
    "train_data['day'] = list(map(str, train_data['day']))\n",
    "train_data['hour'] = list(map(str, train_data['hour']))\n",
    "train_data['Country'] = list(map(str, train_data['Country']))\n",
    "train_data['region'] = list(map(str, train_data['region']))\n",
    "train_data['region_incomeLevel'] = list(map(str, train_data['region_incomeLevel']))\n",
    "\n",
    "test_data['Male'] = list(map(str, test_data['Male']))\n",
    "test_data['month'] = list(map(str, test_data['month']))\n",
    "test_data['day'] = list(map(str, test_data['day']))\n",
    "test_data['hour'] = list(map(str, test_data['hour']))\n",
    "test_data['Country'] = list(map(str, test_data['Country']))\n",
    "test_data['region'] = list(map(str, test_data['region']))\n",
    "test_data['region_incomeLevel'] = list(map(str, test_data['region_incomeLevel']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cc10991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of the numerical feature names.\n",
    "NUMERIC_FEATURE_NAMES = [\n",
    "    \"Daily Time Spent on Site\",\n",
    "    \"Age\",\n",
    "    \"Area Income\",\n",
    "    \"Daily Internet Usage\"\n",
    "]\n",
    "# A dictionary of the categorical features and their vocabulary.\n",
    "CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n",
    "    \"Male\": sorted(list(train_data[\"Male\"].unique())),\n",
    "    \"Country\": sorted(list(train_data[\"Country\"].unique())),\n",
    "    \"region\": sorted(list(train_data[\"region\"].unique())),\n",
    "    \"region_incomeLevel\": sorted(list(train_data[\"region_incomeLevel\"].unique())),\n",
    "    \"month\": sorted(list(train_data[\"month\"].unique())),\n",
    "    \"day\": sorted(list(train_data[\"day\"].unique())),\n",
    "    \"hour\": sorted(list(train_data[\"hour\"].unique()))\n",
    "}\n",
    "\n",
    "# Name of the column to be used as instances weight.\n",
    "# WEIGHT_COLUMN_NAME = \"Area Income\"\n",
    "# A list of the categorical feature names.\n",
    "CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURES_WITH_VOCABULARY.keys())\n",
    "# A list of all the input features.\n",
    "FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n",
    "# A list of column default values for each feature.\n",
    "COLUMN_DEFAULTS = [\n",
    "    [0.0] if feature_name in NUMERIC_FEATURE_NAMES else [\"NA\"]\n",
    "    for feature_name in CSV_HEADER\n",
    "]\n",
    "# The name of the target feature.\n",
    "TARGET_FEATURE_NAME = \"Clicked on Ad\"\n",
    "# A list of the labels of the target features.\n",
    "TARGET_LABELS = ['0', '1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "552da795",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 0.0001\n",
    "DROPOUT_RATE = 0.5\n",
    "BATCH_SIZE = 265\n",
    "NUM_EPOCHS = 30\n",
    "\n",
    "NUM_TRANSFORMER_BLOCKS = 5  # Number of transformer blocks.\n",
    "NUM_HEADS = 4  # Number of attention heads.\n",
    "EMBEDDING_DIMS = 32  # Embedding dimensions of the categorical features.\n",
    "MLP_HIDDEN_UNITS_FACTORS = [\n",
    "    2,\n",
    "    1,\n",
    "]  # MLP hidden layer units, as factors of the number of inputs.\n",
    "NUM_MLP_BLOCKS = 5  # Number of MLP blocks in the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "84254104",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_label_lookup = layers.StringLookup(\n",
    "    vocabulary=TARGET_LABELS, mask_token=None, num_oov_indices=1\n",
    ")\n",
    "\n",
    "\n",
    "def prepare_example(features, target):\n",
    "    target_index = target_label_lookup(target)\n",
    "#     weights = features.pop(WEIGHT_COLUMN_NAME)\n",
    "    return features, target_index\n",
    "\n",
    "\n",
    "def get_dataset_from_csv(csv_file_path, batch_size=128, shuffle=False):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        csv_file_path,\n",
    "        batch_size=batch_size,\n",
    "        column_names=CSV_HEADER,\n",
    "        column_defaults=COLUMN_DEFAULTS,\n",
    "        label_name=TARGET_FEATURE_NAME,\n",
    "        num_epochs=1,\n",
    "#         header=False,\n",
    "#         na_value=\"?\",\n",
    "        shuffle=shuffle,\n",
    "    ).map(prepare_example, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n",
    "    return dataset.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a4efc1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6bf7c530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "    model,\n",
    "    train_data_file,\n",
    "    test_data_file,\n",
    "    num_epochs,\n",
    "    learning_rate,\n",
    "    weight_decay,\n",
    "    batch_size,\n",
    "):\n",
    "\n",
    "#     optimizer = tfa.optimizers.AdamW(\n",
    "#         learning_rate=learning_rate, weight_decay=weight_decay\n",
    "#     )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['acc',\n",
    "                f1_m, precision_m, recall_m],\n",
    "    )\n",
    "\n",
    "    train_dataset = get_dataset_from_csv(train_data_file, batch_size, shuffle=True)\n",
    "    validation_dataset = get_dataset_from_csv(test_data_file, batch_size)\n",
    "\n",
    "    print(\"Start training the model...\")\n",
    "    history = model.fit(\n",
    "        train_dataset, epochs=num_epochs, validation_data=validation_dataset\n",
    "    )\n",
    "    print(\"Model training finished\")\n",
    "\n",
    "    loss, accuracy, f1_score, precision, recall = model.evaluate(validation_dataset, verbose=0)\n",
    "\n",
    "    print(f\"Validation accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f'precision: {precision}')          \n",
    "    print(f'recall: {recall}')\n",
    "    print(f'f1_score: {f1_score}')\n",
    "\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6ae02d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_inputs():\n",
    "    inputs = {}\n",
    "    for feature_name in FEATURE_NAMES:\n",
    "        if feature_name in NUMERIC_FEATURE_NAMES:\n",
    "            inputs[feature_name] = layers.Input(\n",
    "                name=feature_name, shape=(), dtype=tf.float32\n",
    "            )\n",
    "        else:\n",
    "            inputs[feature_name] = layers.Input(\n",
    "                name=feature_name, shape=(), dtype=tf.string\n",
    "            )\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8e1f3a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_inputs(inputs, embedding_dims):\n",
    "\n",
    "    encoded_categorical_feature_list = []\n",
    "    numerical_feature_list = []\n",
    "\n",
    "    for feature_name in inputs:\n",
    "        if feature_name in CATEGORICAL_FEATURE_NAMES:\n",
    "\n",
    "            # Get the vocabulary of the categorical feature.\n",
    "            vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
    "\n",
    "            # Create a lookup to convert string values to an integer indices.\n",
    "            # Since we are not using a mask token nor expecting any out of vocabulary\n",
    "            # (oov) token, we set mask_token to None and  num_oov_indices to 0.\n",
    "            lookup = layers.StringLookup(\n",
    "                vocabulary=vocabulary,\n",
    "                mask_token=None,\n",
    "                num_oov_indices=1,\n",
    "                output_mode=\"int\",\n",
    "            )\n",
    "\n",
    "            # Convert the string input values into integer indices.\n",
    "            encoded_feature = lookup(inputs[feature_name])\n",
    "\n",
    "            # Create an embedding layer with the specified dimensions.\n",
    "            embedding = layers.Embedding(\n",
    "                input_dim=len(vocabulary), output_dim=embedding_dims\n",
    "            )\n",
    "\n",
    "            # Convert the index values to embedding representations.\n",
    "            encoded_categorical_feature = embedding(encoded_feature)\n",
    "            encoded_categorical_feature_list.append(encoded_categorical_feature)\n",
    "\n",
    "        else:\n",
    "\n",
    "            # Use the numerical features as-is.\n",
    "            numerical_feature = tf.expand_dims(inputs[feature_name], -1)\n",
    "            numerical_feature_list.append(numerical_feature)\n",
    "\n",
    "    return encoded_categorical_feature_list, numerical_feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "facffa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp(hidden_units, dropout_rate, activation, normalization_layer, name=None):\n",
    "\n",
    "    mlp_layers = []\n",
    "    for units in hidden_units:\n",
    "        mlp_layers.append(normalization_layer),\n",
    "        mlp_layers.append(layers.Dense(units, activation=activation))\n",
    "        mlp_layers.append(layers.Dropout(dropout_rate))\n",
    "\n",
    "    return keras.Sequential(mlp_layers, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ac29fc64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total model weights: 482925\n",
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "def create_baseline_model(\n",
    "    embedding_dims, num_mlp_blocks, mlp_hidden_units_factors, dropout_rate\n",
    "):\n",
    "\n",
    "    # Create model inputs.\n",
    "    inputs = create_model_inputs()\n",
    "    # encode features.\n",
    "    encoded_categorical_feature_list, numerical_feature_list = encode_inputs(\n",
    "        inputs, embedding_dims\n",
    "    )\n",
    "    # Concatenate all features.\n",
    "    features = layers.concatenate(\n",
    "        encoded_categorical_feature_list + numerical_feature_list\n",
    "    )\n",
    "    # Compute Feedforward layer units.\n",
    "    feedforward_units = [features.shape[-1]]\n",
    "\n",
    "    # Create several feedforwad layers with skip connections.\n",
    "    for layer_idx in range(num_mlp_blocks):\n",
    "        features = create_mlp(\n",
    "            hidden_units=feedforward_units,\n",
    "            dropout_rate=dropout_rate,\n",
    "            activation=keras.activations.gelu,\n",
    "            normalization_layer=layers.LayerNormalization(epsilon=1e-6),\n",
    "            name=f\"feedforward_{layer_idx}\",\n",
    "        )(features)\n",
    "\n",
    "    # Compute MLP hidden_units.\n",
    "    mlp_hidden_units = [\n",
    "        factor * features.shape[-1] for factor in mlp_hidden_units_factors\n",
    "    ]\n",
    "    # Create final MLP.\n",
    "    features = create_mlp(\n",
    "        hidden_units=mlp_hidden_units,\n",
    "        dropout_rate=dropout_rate,\n",
    "        activation=keras.activations.selu,\n",
    "        normalization_layer=layers.BatchNormalization(),\n",
    "        name=\"MLP\",\n",
    "    )(features)\n",
    "\n",
    "    # Add a sigmoid as a binary classifer.\n",
    "    outputs = layers.Dense(units=1, activation=\"sigmoid\", name=\"sigmoid\")(features)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "baseline_model = create_baseline_model(\n",
    "    embedding_dims=EMBEDDING_DIMS,\n",
    "    num_mlp_blocks=NUM_MLP_BLOCKS,\n",
    "    mlp_hidden_units_factors=MLP_HIDDEN_UNITS_FACTORS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    ")\n",
    "\n",
    "print(\"Total model weights:\", baseline_model.count_params())\n",
    "keras.utils.plot_model(baseline_model, show_shapes=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "52a62ca5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training the model...\n",
      "Epoch 1/30\n",
      "4/4 [==============================] - 3s 187ms/step - loss: 0.9217 - acc: 0.2550 - f1_m: 0.7513 - precision_m: 1.1333 - recall_m: 0.5642 - val_loss: 0.0463 - val_acc: 0.5550 - val_f1_m: 1.0000 - val_precision_m: 1.0000 - val_recall_m: 1.0000\n",
      "Epoch 2/30\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.8549 - acc: 0.2862 - f1_m: 0.8526 - precision_m: 1.1126 - recall_m: 0.6972 - val_loss: -0.5285 - val_acc: 0.5550 - val_f1_m: 1.0000 - val_precision_m: 1.0000 - val_recall_m: 1.0000\n",
      "Epoch 3/30\n",
      "4/4 [==============================] - 0s 43ms/step - loss: 0.4233 - acc: 0.3125 - f1_m: 0.7961 - precision_m: 1.0964 - recall_m: 0.6321 - val_loss: -0.6381 - val_acc: 0.5550 - val_f1_m: 1.0000 - val_precision_m: 1.0000 - val_recall_m: 1.0000\n",
      "Epoch 4/30\n",
      "4/4 [==============================] - 0s 41ms/step - loss: 0.1010 - acc: 0.3313 - f1_m: 0.9501 - precision_m: 1.1428 - recall_m: 0.8151 - val_loss: -0.6576 - val_acc: 0.5550 - val_f1_m: 1.0000 - val_precision_m: 1.0000 - val_recall_m: 1.0000\n",
      "Epoch 5/30\n",
      "4/4 [==============================] - 0s 41ms/step - loss: -0.1817 - acc: 0.3638 - f1_m: 0.8964 - precision_m: 1.0421 - recall_m: 0.7868 - val_loss: -0.7779 - val_acc: 0.5550 - val_f1_m: 1.0000 - val_precision_m: 1.0000 - val_recall_m: 1.0000\n",
      "Epoch 6/30\n",
      "4/4 [==============================] - 0s 40ms/step - loss: -0.4874 - acc: 0.3638 - f1_m: 0.9346 - precision_m: 1.0505 - recall_m: 0.8481 - val_loss: -0.6937 - val_acc: 0.5550 - val_f1_m: 1.0000 - val_precision_m: 1.0000 - val_recall_m: 1.0000\n",
      "Epoch 7/30\n",
      "4/4 [==============================] - 0s 41ms/step - loss: -0.8069 - acc: 0.3862 - f1_m: 0.9208 - precision_m: 1.0353 - recall_m: 0.8292 - val_loss: -0.5900 - val_acc: 0.5550 - val_f1_m: 1.0000 - val_precision_m: 1.0000 - val_recall_m: 1.0000\n",
      "Epoch 8/30\n",
      "4/4 [==============================] - 0s 44ms/step - loss: -1.2105 - acc: 0.3975 - f1_m: 0.9830 - precision_m: 1.0886 - recall_m: 0.8962 - val_loss: -0.4628 - val_acc: 0.5550 - val_f1_m: 1.0000 - val_precision_m: 1.0000 - val_recall_m: 1.0000\n",
      "Epoch 9/30\n",
      "4/4 [==============================] - 0s 44ms/step - loss: -1.3576 - acc: 0.4025 - f1_m: 0.9502 - precision_m: 1.0147 - recall_m: 0.8962 - val_loss: -0.4433 - val_acc: 0.5550 - val_f1_m: 1.0000 - val_precision_m: 1.0000 - val_recall_m: 1.0000\n",
      "Epoch 10/30\n",
      "4/4 [==============================] - 0s 42ms/step - loss: -1.5902 - acc: 0.4150 - f1_m: 0.9351 - precision_m: 1.0220 - recall_m: 0.8623 - val_loss: -0.5207 - val_acc: 0.5550 - val_f1_m: 1.0000 - val_precision_m: 1.0000 - val_recall_m: 1.0000\n",
      "Epoch 11/30\n",
      "4/4 [==============================] - 0s 43ms/step - loss: -2.0670 - acc: 0.4263 - f1_m: 0.9687 - precision_m: 1.0229 - recall_m: 0.9217 - val_loss: -0.7378 - val_acc: 0.5550 - val_f1_m: 1.0000 - val_precision_m: 1.0000 - val_recall_m: 1.0000\n",
      "Epoch 12/30\n",
      "4/4 [==============================] - 0s 43ms/step - loss: -2.6416 - acc: 0.4350 - f1_m: 0.9499 - precision_m: 1.0167 - recall_m: 0.8925 - val_loss: -0.8293 - val_acc: 0.5550 - val_f1_m: 1.0000 - val_precision_m: 1.0000 - val_recall_m: 1.0000\n",
      "Epoch 13/30\n",
      "4/4 [==============================] - 0s 41ms/step - loss: -2.9071 - acc: 0.4400 - f1_m: 1.0039 - precision_m: 1.0760 - recall_m: 0.9425 - val_loss: -1.1460 - val_acc: 0.5550 - val_f1_m: 1.0000 - val_precision_m: 1.0000 - val_recall_m: 1.0000\n",
      "Epoch 14/30\n",
      "4/4 [==============================] - 0s 40ms/step - loss: -3.4747 - acc: 0.4375 - f1_m: 0.9757 - precision_m: 1.0125 - recall_m: 0.9425 - val_loss: -1.8236 - val_acc: 0.5550 - val_f1_m: 1.0000 - val_precision_m: 1.0000 - val_recall_m: 1.0000\n",
      "Epoch 15/30\n",
      "4/4 [==============================] - 0s 42ms/step - loss: -4.2872 - acc: 0.4500 - f1_m: 0.9189 - precision_m: 1.0102 - recall_m: 0.8547 - val_loss: -2.1126 - val_acc: 0.5550 - val_f1_m: 1.0000 - val_precision_m: 1.0000 - val_recall_m: 1.0000\n",
      "Epoch 16/30\n",
      "4/4 [==============================] - 0s 43ms/step - loss: -4.9110 - acc: 0.4475 - f1_m: 0.9500 - precision_m: 1.0124 - recall_m: 0.8962 - val_loss: -2.6004 - val_acc: 0.5550 - val_f1_m: 1.0000 - val_precision_m: 1.0000 - val_recall_m: 1.0000\n",
      "Epoch 17/30\n",
      "4/4 [==============================] - 0s 41ms/step - loss: -6.5540 - acc: 0.4625 - f1_m: 0.9860 - precision_m: 1.0040 - recall_m: 0.9689 - val_loss: -3.7195 - val_acc: 0.5550 - val_f1_m: 1.0000 - val_precision_m: 1.0000 - val_recall_m: 1.0000\n",
      "Epoch 18/30\n",
      "4/4 [==============================] - 0s 42ms/step - loss: -7.3188 - acc: 0.4625 - f1_m: 0.9855 - precision_m: 1.0030 - recall_m: 0.9689 - val_loss: -6.6965 - val_acc: 0.5550 - val_f1_m: 1.0000 - val_precision_m: 1.0000 - val_recall_m: 1.0000\n",
      "Epoch 19/30\n",
      "4/4 [==============================] - 0s 42ms/step - loss: -8.8723 - acc: 0.4675 - f1_m: 0.9612 - precision_m: 1.0039 - recall_m: 0.9245 - val_loss: -9.5037 - val_acc: 0.5550 - val_f1_m: 1.0000 - val_precision_m: 1.0000 - val_recall_m: 1.0000\n",
      "Epoch 20/30\n",
      "4/4 [==============================] - 0s 43ms/step - loss: -10.6747 - acc: 0.4688 - f1_m: 0.9884 - precision_m: 1.0049 - recall_m: 0.9726 - val_loss: -11.8258 - val_acc: 0.5550 - val_f1_m: 1.0000 - val_precision_m: 1.0000 - val_recall_m: 1.0000\n",
      "Epoch 21/30\n",
      "4/4 [==============================] - 0s 42ms/step - loss: -13.3747 - acc: 0.4638 - f1_m: 0.9899 - precision_m: 1.0039 - recall_m: 0.9764 - val_loss: -13.4497 - val_acc: 0.5550 - val_f1_m: 1.0000 - val_precision_m: 1.0000 - val_recall_m: 1.0000\n",
      "Epoch 22/30\n",
      "4/4 [==============================] - 0s 41ms/step - loss: -16.2538 - acc: 0.4663 - f1_m: 0.9875 - precision_m: 1.0010 - recall_m: 0.9745 - val_loss: -14.2819 - val_acc: 0.5550 - val_f1_m: 1.0000 - val_precision_m: 1.0000 - val_recall_m: 1.0000\n",
      "Epoch 23/30\n",
      "4/4 [==============================] - 0s 41ms/step - loss: -20.6256 - acc: 0.4775 - f1_m: 0.9952 - precision_m: 1.0019 - recall_m: 0.9887 - val_loss: -13.1679 - val_acc: 0.5550 - val_f1_m: 1.0000 - val_precision_m: 1.0000 - val_recall_m: 1.0000\n",
      "Epoch 24/30\n",
      "4/4 [==============================] - 0s 42ms/step - loss: -25.2105 - acc: 0.4800 - f1_m: 0.9943 - precision_m: 1.0010 - recall_m: 0.9877 - val_loss: -8.0863 - val_acc: 0.5550 - val_f1_m: 1.0000 - val_precision_m: 1.0000 - val_recall_m: 1.0000\n",
      "Epoch 25/30\n",
      "4/4 [==============================] - 0s 41ms/step - loss: -30.6474 - acc: 0.4837 - f1_m: 0.9967 - precision_m: 1.0000 - recall_m: 0.9934 - val_loss: -2.8088 - val_acc: 0.5550 - val_f1_m: 1.0000 - val_precision_m: 1.0000 - val_recall_m: 1.0000\n",
      "Epoch 26/30\n",
      "4/4 [==============================] - 0s 41ms/step - loss: -40.5956 - acc: 0.4825 - f1_m: 0.9981 - precision_m: 1.0000 - recall_m: 0.9962 - val_loss: -5.5675 - val_acc: 0.5550 - val_f1_m: 1.0000 - val_precision_m: 1.0000 - val_recall_m: 1.0000\n",
      "Epoch 27/30\n",
      "4/4 [==============================] - 0s 40ms/step - loss: -52.2817 - acc: 0.4800 - f1_m: 0.9972 - precision_m: 1.0000 - recall_m: 0.9943 - val_loss: -12.7914 - val_acc: 0.5550 - val_f1_m: 1.0000 - val_precision_m: 1.0000 - val_recall_m: 1.0000\n",
      "Epoch 28/30\n",
      "4/4 [==============================] - 0s 41ms/step - loss: -65.6727 - acc: 0.4812 - f1_m: 0.9962 - precision_m: 1.0000 - recall_m: 0.9925 - val_loss: -9.1638 - val_acc: 0.5550 - val_f1_m: 1.0000 - val_precision_m: 1.0000 - val_recall_m: 1.0000\n",
      "Epoch 29/30\n",
      "4/4 [==============================] - 0s 41ms/step - loss: -86.2106 - acc: 0.4825 - f1_m: 0.9981 - precision_m: 1.0010 - recall_m: 0.9953 - val_loss: 19.9476 - val_acc: 0.0000e+00 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Epoch 30/30\n",
      "4/4 [==============================] - 0s 40ms/step - loss: -112.6785 - acc: 0.4837 - f1_m: 0.9986 - precision_m: 1.0000 - recall_m: 0.9972 - val_loss: 8.7606 - val_acc: 0.0000e+00 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
      "Model training finished\n",
      "Validation accuracy: 0.0%\n",
      "precision: 0.0\n",
      "recall: 0.0\n",
      "f1_score: 0.0\n"
     ]
    }
   ],
   "source": [
    "history, model = run_experiment(\n",
    "    model=baseline_model,\n",
    "    train_data_file=train_data_file,\n",
    "    test_data_file=test_data_file,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7b0824f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-5263.31103515625, 0.5550000071525574, 1.0, 1.0]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset = get_dataset_from_csv(test_data_file, 128)\n",
    "result = model.evaluate(validation_dataset, verbose=0)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "edec62bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total model weights: 247977\n",
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    }
   ],
   "source": [
    "def create_tabtransformer_classifier(\n",
    "    num_transformer_blocks,\n",
    "    num_heads,\n",
    "    embedding_dims,\n",
    "    mlp_hidden_units_factors,\n",
    "    dropout_rate,\n",
    "    use_column_embedding=False,\n",
    "):\n",
    "\n",
    "    # Create model inputs.\n",
    "    inputs = create_model_inputs()\n",
    "    # encode features.\n",
    "    encoded_categorical_feature_list, numerical_feature_list = encode_inputs(\n",
    "        inputs, embedding_dims\n",
    "    )\n",
    "    # Stack categorical feature embeddings for the Tansformer.\n",
    "    encoded_categorical_features = tf.stack(encoded_categorical_feature_list, axis=1)\n",
    "    # Concatenate numerical features.\n",
    "    numerical_features = layers.concatenate(numerical_feature_list)\n",
    "\n",
    "    # Add column embedding to categorical feature embeddings.\n",
    "    if use_column_embedding:\n",
    "        num_columns = encoded_categorical_features.shape[1]\n",
    "        column_embedding = layers.Embedding(\n",
    "            input_dim=num_columns, output_dim=embedding_dims\n",
    "        )\n",
    "        column_indices = tf.range(start=0, limit=num_columns, delta=1)\n",
    "        encoded_categorical_features = encoded_categorical_features + column_embedding(\n",
    "            column_indices\n",
    "        )\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for block_idx in range(num_transformer_blocks):\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embedding_dims,\n",
    "            dropout=dropout_rate,\n",
    "            name=f\"multihead_attention_{block_idx}\",\n",
    "        )(encoded_categorical_features, encoded_categorical_features)\n",
    "        # Skip connection 1.\n",
    "        x = layers.Add(name=f\"skip_connection1_{block_idx}\")(\n",
    "            [attention_output, encoded_categorical_features]\n",
    "        )\n",
    "        # Layer normalization 1.\n",
    "        x = layers.LayerNormalization(name=f\"layer_norm1_{block_idx}\", epsilon=1e-6)(x)\n",
    "        # Feedforward.\n",
    "        feedforward_output = create_mlp(\n",
    "            hidden_units=[embedding_dims],\n",
    "            dropout_rate=dropout_rate,\n",
    "            activation=keras.activations.gelu,\n",
    "            normalization_layer=layers.LayerNormalization(epsilon=1e-6),\n",
    "            name=f\"feedforward_{block_idx}\",\n",
    "        )(x)\n",
    "        # Skip connection 2.\n",
    "        x = layers.Add(name=f\"skip_connection2_{block_idx}\")([feedforward_output, x])\n",
    "        # Layer normalization 2.\n",
    "        encoded_categorical_features = layers.LayerNormalization(\n",
    "            name=f\"layer_norm2_{block_idx}\", epsilon=1e-6\n",
    "        )(x)\n",
    "\n",
    "    # Flatten the \"contextualized\" embeddings of the categorical features.\n",
    "    categorical_features = layers.Flatten()(encoded_categorical_features)\n",
    "    # Apply layer normalization to the numerical features.\n",
    "    numerical_features = layers.LayerNormalization(epsilon=1e-6)(numerical_features)\n",
    "    # Prepare the input for the final MLP block.\n",
    "    features = layers.concatenate([categorical_features, numerical_features])\n",
    "\n",
    "    # Compute MLP hidden_units.\n",
    "    mlp_hidden_units = [\n",
    "        factor * features.shape[-1] for factor in mlp_hidden_units_factors\n",
    "    ]\n",
    "    # Create final MLP.\n",
    "    features = create_mlp(\n",
    "        hidden_units=mlp_hidden_units,\n",
    "        dropout_rate=dropout_rate,\n",
    "        activation=keras.activations.selu,\n",
    "        normalization_layer=layers.BatchNormalization(),\n",
    "        name=\"MLP\",\n",
    "    )(features)\n",
    "\n",
    "    # Add a sigmoid as a binary classifer.\n",
    "    outputs = layers.Dense(units=1, activation=\"sigmoid\", name=\"sigmoid\")(features)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "tabtransformer_model = create_tabtransformer_classifier(\n",
    "    num_transformer_blocks=NUM_TRANSFORMER_BLOCKS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    embedding_dims=EMBEDDING_DIMS,\n",
    "    mlp_hidden_units_factors=MLP_HIDDEN_UNITS_FACTORS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    ")\n",
    "\n",
    "print(\"Total model weights:\", tabtransformer_model.count_params())\n",
    "keras.utils.plot_model(tabtransformer_model, show_shapes=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cdd55c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training the model...\n",
      "Epoch 1/30\n",
      "4/4 [==============================] - 7s 353ms/step - loss: 1.2528 - accuracy: 0.4963 - val_loss: 0.9202 - val_accuracy: 0.4600\n",
      "Epoch 2/30\n",
      "4/4 [==============================] - 0s 82ms/step - loss: 1.2125 - accuracy: 0.5188 - val_loss: 0.9536 - val_accuracy: 0.5050\n",
      "Epoch 3/30\n",
      "4/4 [==============================] - 0s 83ms/step - loss: 1.2886 - accuracy: 0.4950 - val_loss: 1.0324 - val_accuracy: 0.5350\n",
      "Epoch 4/30\n",
      "4/4 [==============================] - 0s 84ms/step - loss: 1.1514 - accuracy: 0.5288 - val_loss: 1.0774 - val_accuracy: 0.5150\n",
      "Epoch 5/30\n",
      "4/4 [==============================] - 0s 82ms/step - loss: 1.2495 - accuracy: 0.5213 - val_loss: 0.9806 - val_accuracy: 0.5100\n",
      "Epoch 6/30\n",
      "4/4 [==============================] - 0s 79ms/step - loss: 1.1659 - accuracy: 0.4988 - val_loss: 0.8540 - val_accuracy: 0.4900\n",
      "Epoch 7/30\n",
      "4/4 [==============================] - 0s 83ms/step - loss: 1.1503 - accuracy: 0.5075 - val_loss: 0.8032 - val_accuracy: 0.4800\n",
      "Epoch 8/30\n",
      "4/4 [==============================] - 0s 80ms/step - loss: 1.1879 - accuracy: 0.4837 - val_loss: 0.8279 - val_accuracy: 0.4600\n",
      "Epoch 9/30\n",
      "4/4 [==============================] - 0s 84ms/step - loss: 1.1346 - accuracy: 0.5038 - val_loss: 0.8176 - val_accuracy: 0.5050\n",
      "Epoch 10/30\n",
      "4/4 [==============================] - 0s 87ms/step - loss: 1.1720 - accuracy: 0.4837 - val_loss: 0.7909 - val_accuracy: 0.5150\n",
      "Epoch 11/30\n",
      "4/4 [==============================] - 0s 88ms/step - loss: 1.0990 - accuracy: 0.5288 - val_loss: 0.7496 - val_accuracy: 0.5050\n",
      "Epoch 12/30\n",
      "4/4 [==============================] - 0s 87ms/step - loss: 1.2335 - accuracy: 0.4462 - val_loss: 0.7254 - val_accuracy: 0.4650\n",
      "Epoch 13/30\n",
      "4/4 [==============================] - 0s 76ms/step - loss: 1.0690 - accuracy: 0.5400 - val_loss: 0.7141 - val_accuracy: 0.5250\n",
      "Epoch 14/30\n",
      "4/4 [==============================] - 0s 81ms/step - loss: 1.0896 - accuracy: 0.5225 - val_loss: 0.7321 - val_accuracy: 0.5400\n",
      "Epoch 15/30\n",
      "4/4 [==============================] - 0s 79ms/step - loss: 1.0780 - accuracy: 0.5163 - val_loss: 0.7734 - val_accuracy: 0.5350\n",
      "Epoch 16/30\n",
      "4/4 [==============================] - 0s 82ms/step - loss: 1.1493 - accuracy: 0.5138 - val_loss: 0.7599 - val_accuracy: 0.5500\n",
      "Epoch 17/30\n",
      "4/4 [==============================] - 0s 78ms/step - loss: 1.1218 - accuracy: 0.5000 - val_loss: 0.7151 - val_accuracy: 0.5300\n",
      "Epoch 18/30\n",
      "4/4 [==============================] - 0s 80ms/step - loss: 1.0588 - accuracy: 0.5213 - val_loss: 0.7008 - val_accuracy: 0.4950\n",
      "Epoch 19/30\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 1.0880 - accuracy: 0.4988 - val_loss: 0.7065 - val_accuracy: 0.5200\n",
      "Epoch 20/30\n",
      "4/4 [==============================] - 0s 89ms/step - loss: 1.0922 - accuracy: 0.4988 - val_loss: 0.7142 - val_accuracy: 0.5200\n",
      "Epoch 21/30\n",
      "4/4 [==============================] - 0s 92ms/step - loss: 0.9919 - accuracy: 0.5475 - val_loss: 0.7217 - val_accuracy: 0.5200\n",
      "Epoch 22/30\n",
      "4/4 [==============================] - 0s 77ms/step - loss: 1.0270 - accuracy: 0.5225 - val_loss: 0.7458 - val_accuracy: 0.4800\n",
      "Epoch 23/30\n",
      "4/4 [==============================] - 0s 78ms/step - loss: 1.0326 - accuracy: 0.5200 - val_loss: 0.7733 - val_accuracy: 0.4600\n",
      "Epoch 24/30\n",
      "4/4 [==============================] - 0s 79ms/step - loss: 0.9874 - accuracy: 0.5125 - val_loss: 0.7972 - val_accuracy: 0.4600\n",
      "Epoch 25/30\n",
      "4/4 [==============================] - 0s 80ms/step - loss: 0.9839 - accuracy: 0.5163 - val_loss: 0.8400 - val_accuracy: 0.4500\n",
      "Epoch 26/30\n",
      "4/4 [==============================] - 0s 81ms/step - loss: 1.0446 - accuracy: 0.5013 - val_loss: 0.9004 - val_accuracy: 0.4450\n",
      "Epoch 27/30\n",
      "4/4 [==============================] - 0s 79ms/step - loss: 1.0398 - accuracy: 0.5288 - val_loss: 0.8950 - val_accuracy: 0.4450\n",
      "Epoch 28/30\n",
      "4/4 [==============================] - 0s 85ms/step - loss: 0.9892 - accuracy: 0.5325 - val_loss: 0.8309 - val_accuracy: 0.4700\n",
      "Epoch 29/30\n",
      "4/4 [==============================] - 0s 79ms/step - loss: 1.0142 - accuracy: 0.5175 - val_loss: 0.7964 - val_accuracy: 0.4650\n",
      "Epoch 30/30\n",
      "4/4 [==============================] - 0s 84ms/step - loss: 1.0341 - accuracy: 0.5275 - val_loss: 0.7894 - val_accuracy: 0.4300\n",
      "Model training finished\n",
      "Validation accuracy: 43.0%\n"
     ]
    }
   ],
   "source": [
    "history = run_experiment(\n",
    "    model=tabtransformer_model,\n",
    "    train_data_file=train_data_file,\n",
    "    test_data_file=test_data_file,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242185fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
